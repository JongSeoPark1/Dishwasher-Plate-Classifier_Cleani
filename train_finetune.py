# train_finetune.py
import os
import copy
import torch
import torch.nn as nn
import torch.optim as optim
from dataset import get_dataloaders
from model_loader import get_model

# 1. ì„¤ì • (ê²½ë¡œë§Œ ë‹¤ë¦…ë‹ˆë‹¤!)
DATA_DIR = './data/finetune'      # ğŸ‘ˆ íŒŒì¸íŠœë‹ ë°ì´í„° ê²½ë¡œ
LOAD_DIR = './saved_models'       # ğŸ‘ˆ 1ì°¨ í•™ìŠµëœ ëª¨ë¸ ê°€ì ¸ì˜¤ëŠ” ê³³
SAVE_DIR = './finetuned_models'   # ğŸ‘ˆ ìµœì¢… ëª¨ë¸ ì €ì¥í•˜ëŠ” ê³³
MODEL_NAMES = ['mobilenet', 'efficientnet', 'resnet']
NUM_EPOCHS = 30
LR = 1e-5        # ğŸ‘ˆ ì•„ì£¼ ì‘ì€ í•™ìŠµë¥  (ì§€ì‹ ë³´ì¡´)

def main():
    if torch.backends.mps.is_available(): device = torch.device("mps")
    elif torch.cuda.is_available(): device = torch.device("cuda")
    else: device = torch.device("cpu")

    # ë°ì´í„°ì…‹ ë¡œë“œ
    if not os.path.exists(DATA_DIR):
        print(f"âŒ íŒŒì¸íŠœë‹ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {DATA_DIR}")
        return
    dataloaders, dataset_sizes, class_names = get_dataloaders(DATA_DIR)
    print(f"â™»ï¸ [íŒŒì¸ íŠœë‹] ë°ì´í„°: {DATA_DIR} | í´ë˜ìŠ¤: {class_names}")

    os.makedirs(SAVE_DIR, exist_ok=True)

    for name in MODEL_NAMES:
        print(f"\nğŸ”„ [{name}] íŒŒì¸íŠœë‹ ì‹œì‘...")
        
        # ëª¨ë¸ ìƒì„±
        model = get_model(name, len(class_names), device)
        
        # â˜… 1ì°¨ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸°
        load_path = os.path.join(LOAD_DIR, f"{name}_best.pth")
        if os.path.exists(load_path):
            model.load_state_dict(torch.load(load_path, map_location=device))
            print(f"  âœ… 1ì°¨ í•™ìŠµ ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {load_path}")
        else:
            print(f"  âš ï¸ 1ì°¨ í•™ìŠµ íŒŒì¼ì´ ì—†ì–´ ì²˜ìŒë¶€í„° í•™ìŠµí•©ë‹ˆë‹¤.")

        criterion = nn.CrossEntropyLoss()
        optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-2)

        best_acc = 0.0
        best_wts = copy.deepcopy(model.state_dict())

        # (ì´í•˜ í•™ìŠµ ë£¨í”„ëŠ” ë™ì¼, ìƒëµ ê°€ëŠ¥í•˜ì§€ë§Œ ë³µë¶™ í¸ì˜ë¥¼ ìœ„í•´ ìœ ì§€)
        for epoch in range(NUM_EPOCHS):
            for phase in ['train', 'val']:
                if phase == 'train': model.train()
                else: model.eval()
                running_corrects = 0
                for inputs, labels in dataloaders[phase]:
                    inputs, labels = inputs.to(device), labels.to(device)
                    optimizer.zero_grad()
                    with torch.set_grad_enabled(phase == 'train'):
                        outputs = model(inputs)
                        _, preds = torch.max(outputs, 1)
                        loss = criterion(outputs, labels)
                        if phase == 'train':
                            loss.backward()
                            optimizer.step()
                    running_corrects += torch.sum(preds == labels.data)
                
                epoch_acc = running_corrects.float() / dataset_sizes[phase]
                if phase == 'val' and epoch_acc > best_acc:
                    best_acc = epoch_acc
                    best_wts = copy.deepcopy(model.state_dict())
            
            if (epoch+1) % 5 == 0: print(f"Epoch {epoch+1}/{NUM_EPOCHS} ì§„í–‰ ì¤‘...")

        # ìµœì¢… ì €ì¥
        torch.save(best_wts, os.path.join(SAVE_DIR, f"{name}_best.pth"))
        print(f"  ğŸ’¾ íŒŒì¸íŠœë‹ ì™„ë£Œ ë° ì €ì¥ë¨.")

if __name__ == '__main__':
    main()